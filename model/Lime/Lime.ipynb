{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For adjusting 'scales' of train, test1 and test2\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# For hyperparameter optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# For amount_spent modeling\n",
    "import lightgbm as lgb\n",
    "# For survival_time modeling\n",
    "import xgboost as xgb\n",
    "# For screening survival, non-spent users modeling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Customizing functions\n",
    "from MJ_functions import merge_dummy_data, categorical_to_int, return_final_data, return_true_label, return_multi_pred_label\n",
    "from MJ_functions import hypertuning_rscv, plotImp_multioutput, score_function_2, find_best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Import Data and Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data(40000, 997)\n",
    "merged_data = pd.read_csv(\"train_preprocess_1.csv\").copy().sort_values('acc_id').reset_index(drop=True)\n",
    "# Test1 data(40000, 995)\n",
    "test_1 = pd.read_csv(\"test1_preprocess_1.csv\").copy().sort_values('acc_id').reset_index(drop=True)\n",
    "# Test2 data(40000, 995)\n",
    "test_2 = pd.read_csv(\"test2_preprocess_1.csv\").copy().sort_values('acc_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature = ['acc_id'] # User id\n",
    "label_feature = ['survival_time', 'amount_spent', 'survival_yn', 'amount_yn'] # Labels for profit modeling\n",
    "#category = [i for i in merged_data.columns.values if ('common_item_sell'in i)|('common_item_buy'in i)|('sell_time'in i)|('buy_time'in i)|('sell_type'in i)|('buy_type'in i)|('last_sell_item_type'in i)|('last_buy_item_type'in i)]\n",
    "remove_features = ['combat_days']+[i for i in merged_data.columns.values if ('day_1_' in i)|('day_4_' in i)|('day_8_' in i)|('day_17' in i)|('day_20' in i)|('day_21' in i)|('day_22' in i)|('day_23' in i)|('day_24' in i)|('day_25' in i)]\n",
    "features = sorted(list(set(merged_data.columns) - set(user_feature+label_feature+remove_features)))\n",
    "#scale_features = sorted(list(set(features)-set(category)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sorted(list(set(merged_data.columns) - set(user_feature+label_feature+remove_features)))\n",
    "len(features)\n",
    "day_features = [x for x in features if 'day_' in x and '_day_' not in x]\n",
    "wo_day_features = sorted(list(set(features) - set(day_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['survival_yn'] = np.where(merged_data['survival_time']==64, 1, 0)\n",
    "merged_data['amount_yn'] = np.where(merged_data['amount_spent']==0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(merged_data['survival_yn'].value_counts(), merged_data['survival_yn'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Robust Scaling and Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaling == True:\n",
    "    all_data = pd.concat([merged_data, test_1, test_2], sort = True).reset_index(drop=True)\n",
    "    transformer = RobustScaler().fit(all_data[features])\n",
    "\n",
    "    merged_data[features] = transformer.transform(merged_data[features])\n",
    "    test_1[features] = transformer.transform(test_1[features])\n",
    "    test_2[features] = transformer.transform(test_2[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold1 = pd.read_csv('train_fold2.csv')\n",
    "test_fold1 = pd.read_csv('test_fold2.csv')\n",
    "train_fold1_acc_id = train_fold1['acc_id'].reset_index(drop = True)\n",
    "test_fold1_acc_id = test_fold1['acc_id'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = merged_data[merged_data['acc_id'].isin(train_fold1_acc_id)]\n",
    "merged_test = merged_data[merged_data['acc_id'].isin(test_fold1_acc_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Binary Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_yn, y_train_yn = merged_train[wo_day_features], merged_train[['survival_yn', 'amount_yn']]\n",
    "X_test_yn, y_test_yn = merged_test[wo_day_features], merged_test[['survival_yn', 'amount_yn']]\n",
    "\n",
    "multi_rf_clf = MultiOutputClassifier(RandomForestClassifier(n_estimators=100,\n",
    "                                                            random_state=0,\n",
    "                                                            verbose=3,n_jobs = -1)).fit(X_train_yn, y_train_yn)  \n",
    "true_label_yn = return_true_label(y_test_yn, merged_test)\n",
    "pred_label_yn = return_multi_pred_label(multi_rf_clf, true_label_yn, X_test_yn)\n",
    "\n",
    "sur_pred_res = pd.concat([pd.DataFrame(multi_rf_clf.predict_proba(X_test_yn)[0]), pred_label_yn[1][['pred_survival_time', 'survival_time']]], 1)\n",
    "sur_pred_res.columns = ['survival_yn_prob_0', 'survival_yn_prob_1', 'pred_survival_yn', 'survival_yn']\n",
    "ams_pred_res = pd.concat([pd.DataFrame(multi_rf_clf.predict_proba(X_test_yn)[1]), pred_label_yn[1][['pred_amount_spent', 'amount_spent']]], 1)\n",
    "ams_pred_res.columns = ['amount_yn_prob_0', 'amount_yn_prob_1', 'pred_amount_yn', 'amount_yn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Survival Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_array = merged_train[wo_day_features].as_matrix()\n",
    "test_array = merged_test[wo_day_features].as_matrix()\n",
    "rf_lst = list(pd.DataFrame(multi_rf_clf.estimators_[0].predict_proba(X_test_yn)).sort_values(0).index[800:810])+list(pd.DataFrame(multi_rf_clf.estimators_[0].predict_proba(X_test_yn)).sort_values(0).index[7190:7200])\n",
    "for i in rf_lst:\n",
    "    explainer_sur_rf = lime.lime_tabular.LimeTabularExplainer(train_array, feature_names=wo_day_features, class_names=[0,1])\n",
    "    exp_rf_sur = explainer_sur_rf.explain_instance(test_array[i], multi_rf_clf.estimators_[0].predict_proba, num_features = 5)        \n",
    "    exp_rf_sur.show_in_notebook()\n",
    "    print(y_test_yn['survival_yn'].reset_index(drop=True)[i])\n",
    "    for k in exp_rf_sur.as_list():\n",
    "        print(k,sep='\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) amount spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_array = merged_train[wo_day_features].as_matrix()\n",
    "test_array = merged_test[wo_day_features].as_matrix()\n",
    "rf_lst = list(pd.DataFrame(multi_rf_clf.estimators_[1].predict_proba(X_test_yn)).sort_values(0).index[800:810])+list(pd.DataFrame(multi_rf_clf.estimators_[1].predict_proba(X_test_yn)).sort_values(0).index[7190:7200])\n",
    "for i in rf_lst:\n",
    "    explainer_sur_rf = lime.lime_tabular.LimeTabularExplainer(train_array, feature_names=wo_day_features, class_names=[0,1])\n",
    "    exp_rf_sur = explainer_sur_rf.explain_instance(test_array[i], multi_rf_clf.estimators_[1].predict_proba, num_features = 5)        \n",
    "    exp_rf_sur.show_in_notebook()\n",
    "    print(y_test_yn['amount_yn'].reset_index(drop=True)[i])\n",
    "    for k in exp_rf_sur.as_list():\n",
    "        print(k,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) amount spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params= {'boosting_type': 'gbdt',\n",
    " 'class_weight': None,\n",
    " 'colsample_bytree': 0.37758858328507827,\n",
    " 'importance_type': 'split',\n",
    " 'learning_rate': 0.01,\n",
    " 'max_depth': 5,\n",
    " 'min_child_samples': 20,\n",
    " 'min_child_weight': 0.001,\n",
    " 'min_split_gain': 0.0,\n",
    " 'n_estimators': 1000,\n",
    " 'n_jobs': -1,\n",
    " 'num_leaves': 999,\n",
    " 'objective': 'rmse',\n",
    " 'random_state': 42,\n",
    " 'reg_alpha': 9.011935682890176,\n",
    " 'reg_lambda': 1.6839355842740356,\n",
    " 'silent': True,\n",
    " 'subsample': 0.7436506067797896,\n",
    " 'subsample_for_bin': 200000,\n",
    " 'subsample_freq': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = merged_train[wo_day_features], merged_train['amount_spent']\n",
    "X_test, y_test = merged_test[wo_day_features], merged_test['amount_spent']\n",
    "\n",
    "lgbtrain = lgb.Dataset(X_train, label=y_train)\n",
    "lgbval = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "num_rounds =10000\n",
    "early_stopping_rounds=100\n",
    "\n",
    "LGBM_opt_model = lgb.train(opt_params, lgbtrain, num_rounds, valid_sets = lgbval, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "surv_reg = list(pd.DataFrame(LGBM_opt_model.predict(X_test)).sort_values(0,ascending=0).index[800:810])+list(pd.DataFrame(LGBM_opt_model.predict(X_test)).sort_values(0,ascending=0).index[7190:7200])\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, \n",
    "                                                   feature_names=wo_day_features, class_names=['amount_spent'], verbose=True, mode='regression')\n",
    "for i in surv_reg:\n",
    "    exp = explainer.explain_instance(X_test.iloc[i,:], LGBM_opt_model.predict, num_features=7)\n",
    "    exp.show_in_notebook(show_table=True)\n",
    "    for k in exp.as_list():\n",
    "        print(k,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) survival time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a train and validation dmatrices \n",
    "X_train, y_train = merged_train[wo_day_features], merged_train['survival_time']\n",
    "X_test, y_test = merged_test[wo_day_features], merged_test['survival_time']\n",
    "\n",
    "xgb_opt_params = {'colsample_bytree': 0.4281763598169799,\n",
    " 'gamma': 30.0,\n",
    " 'max_depth': 453,\n",
    " 'min_child_weight': 15.229362467690951,\n",
    " 'subsample': 0.7,\n",
    " 'eta': 0.1,\n",
    " 'objective': 'count:poisson'}\n",
    "\n",
    "\n",
    "xgtrain = xgb.DMatrix(X_train, label=y_train,feature_names=wo_day_features)\n",
    "xgval = xgb.DMatrix(X_test, label=y_test,feature_names=wo_day_features)\n",
    "\n",
    "watchlist = [(xgtrain, 'train'),(xgval, 'val')]\n",
    "\n",
    "num_rounds =10000\n",
    "#early_stopping_rounds=50\n",
    "\n",
    "#create a train and validation dmatrices \n",
    "xgtrain = xgb.DMatrix(X_train, label=y_train,feature_names=wo_day_features)\n",
    "xgval = xgb.DMatrix(X_test, label=y_test,feature_names=wo_day_features)\n",
    "\n",
    "watchlist = [(xgtrain, 'train'),(xgval, 'val')]\n",
    "\n",
    "num_rounds =10000\n",
    "#early_stopping_rounds=50\n",
    "\n",
    "rgrs = xgb.train(xgb_opt_params, xgtrain, num_rounds, watchlist, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xglst = list(pd.DataFrame(rgrs.predict(xgval)).sort_values(0).index[800:810])  + list(pd.DataFrame(rgrs.predict(xgval)).sort_values(0).index[7190:7200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped_predict(data_x):\n",
    "    '''\n",
    "    wrap xgboost predict function in order to make it lime-friendly\n",
    "    - model and feature_names are defined outside\n",
    "    '''\n",
    "    \n",
    "    dummy_y = np.array([ 1 for _ in range(data_x.shape[0]) ])\n",
    "    tmp_data = xgb.DMatrix(data_x, dummy_y, feature_names=wo_day_features)\n",
    "    \n",
    "    tmp_out = rgrs.predict(tmp_data)\n",
    "    \n",
    "    '''    # add the first column to make it like predict_proba\n",
    "    out = np.zeros((data_x.shape[0], 2))\n",
    "    out[:, 0] = 1-tmp_out\n",
    "    out[:, 1] = tmp_out'''\n",
    "    \n",
    "    return tmp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, \n",
    "                                                   feature_names=wo_day_features, class_names=['survival_time'], verbose=True, mode='regression')\n",
    "for i in xglst:\n",
    "    exp = explainer.explain_instance(X_test.iloc[i,:], wrapped_predict, num_features=10)\n",
    "    exp.show_in_notebook(show_table=True)\n",
    "    for k in exp.as_list():\n",
    "        print(k,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
